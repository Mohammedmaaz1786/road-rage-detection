{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“‚ 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  2. Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Load the saved model for violence detection\n",
    "model = load_model('violence_detection_model2.h5')\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¥ 3. Live Detection & Frame Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Function for live detection and saving violence frames\n",
    "def live_detection_save_violence_frames(model, video_path, output_dir, buffer_size=30, violence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Perform live violence detection on a video stream and save frames labeled as violence.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained deep learning model\n",
    "    - video_path: Path to the video file (or use 0 for webcam)\n",
    "    - output_dir: Directory to save frames with detected violence\n",
    "    - buffer_size: Number of frames to process in a sequence for prediction\n",
    "    - violence_threshold: Threshold for classifying violence (default = 0.6)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ðŸ“Œ Extract the video name without the extension\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    \n",
    "    # ðŸ“Œ Create a directory for storing detected frames\n",
    "    video_output_dir = os.path.join(output_dir, video_name)\n",
    "    os.makedirs(video_output_dir, exist_ok=True)\n",
    "\n",
    "    # ðŸ“Œ Initialize video capture\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_buffer = []  # Stores frames for batch processing\n",
    "    frame_count = 0    # Track frame numbers\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # ðŸ“Œ Resize the frame to match model input size (150x150)\n",
    "        frame_resized = cv2.resize(frame, (150, 150))\n",
    "        frame_buffer.append(frame_resized)\n",
    "\n",
    "        # ðŸ“Œ Default label & color (for visualization)\n",
    "        label = \"Processing...\"\n",
    "        color = (255, 255, 255)  # White for processing\n",
    "\n",
    "        # ðŸ“Œ Perform prediction once we have enough frames\n",
    "        if len(frame_buffer) == buffer_size:\n",
    "            input_data = np.array(frame_buffer).reshape((1, buffer_size, 150, 150, 3)) / 255.0\n",
    "            prediction = model.predict(input_data)[0][0]\n",
    "\n",
    "            # ðŸ“Œ Apply threshold to determine if violence is present\n",
    "            if prediction > violence_threshold:\n",
    "                label = \"Violence\"\n",
    "                color = (0, 0, 255)  # Red for violence\n",
    "\n",
    "                # ðŸ“Œ Save the frame with the label overlayed\n",
    "                frame_filename = os.path.join(video_output_dir, f\"frame_{frame_count}.jpg\")\n",
    "                frame_with_label = frame.copy()\n",
    "                cv2.putText(frame_with_label, label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, color, 3)\n",
    "                cv2.imwrite(frame_filename, frame_with_label)\n",
    "            else:\n",
    "                label = \"No Violence\"\n",
    "                color = (0, 255, 0)  # Green for no violence\n",
    "\n",
    "            # ðŸ“Œ Remove the oldest frame from buffer (FIFO mechanism)\n",
    "            frame_buffer.pop(0)\n",
    "\n",
    "        # ðŸ“Œ Display the label on the frame\n",
    "        cv2.putText(frame, label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, color, 3)\n",
    "\n",
    "        # ðŸ“Œ Show the live video feed with the detection label\n",
    "        cv2.imshow('Live Violence Detection', frame)\n",
    "\n",
    "        # ðŸ“Œ Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # ðŸ“Œ Release resources\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ 4. Run Live Detection on a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Paths and parameters\n",
    "video_path = 'sample.mp4'  # Replace with your video path\n",
    "output_dir = 'Violent_Frames'  # Directory to save detected frames\n",
    "\n",
    "# ðŸ“Œ Start live detection and save violence frames\n",
    "live_detection_save_violence_frames(model, video_path, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
